# Minimal Wan2.1 i2v LoRA training recipe. Start from this and tweak paths/batch sizes to match your hardware.

output_dir = '/projects_vol/gp_slab/chen2008/lava/training_output'
dataset = '/projects_vol/gp_slab/chen2008/lava/diffusion-pipe/examples/dataset.toml'

# Training settings
epochs = 1000
max_steps = 200 
micro_batch_size_per_gpu = 1
pipeline_stages = 1
gradient_accumulation_steps = 1
gradient_clipping = 1.0
warmup_steps = 10
blocks_to_swap = 20


# Misc
save_every_n_epochs = 20
checkpoint_every_n_epochs = 20
activation_checkpointing = 'unsloth'
partition_method = 'parameters'
save_dtype = 'bfloat16'
caching_batch_size = 1
steps_per_print = 1
video_clip_mode = 'single_beginning'

[model]
type = 'wan'
ckpt_path = '/projects_vol/gp_slab/chen2008/lava/Wan2.1-I2V-14B-480P'
dtype = 'bfloat16'
# fp8 keeps memory in check without hurting convergence for LoRA.
transformer_dtype = 'float8'
timestep_sample_method = 'logit_normal'


[adapter]
type = 'lora'
rank = 32
dtype = 'bfloat16'
target_modules = ['q', 'k', 'v', 'o']

[optimizer]
type = 'AdamW8bitKahan'
lr = 2e-5
betas = [0.9, 0.99]
weight_decay = 0.01
stabilize = false
